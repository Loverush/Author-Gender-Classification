{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T16:03:10.627508Z",
     "start_time": "2019-08-18T16:03:10.615099Z"
    },
    "init_cell": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "### 2.1 Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Mine POS patterns\n",
    "tagList = [\n",
    "    'NN', 'CC', 'LS', 'PDT', 'POS', 'SYM', 'NNS', 'NNP', 'NNPS', 'FW', 'CD',\n",
    "    'JJ', 'JJR', 'JJS', 'IN', 'TO', 'DT', 'EX', 'PRP', 'PRP$', 'WDT', 'WP',\n",
    "    'WP$', 'MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG', 'RB', 'RBR', 'RBS',\n",
    "    'RP', 'WRB', 'UH', '.'\n",
    "]\n",
    "\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './postprocess_blogs/blogs/female/'\n",
    "    else:\n",
    "        txtDir = './postprocess_blogs/blogs/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    blogs_gender = os.listdir(txtDir)\n",
    "    for i in range(0, len(blogs_gender)):\n",
    "        m = blogs_gender[i]\n",
    "        print(\"Processing: \", m)\n",
    "        print(\"Files:\", len(os.listdir(txtDir + m)))\n",
    "        for file in os.listdir(txtDir + m):\n",
    "            text = gettext(txtDir + m + '/' + file)\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            CorpusPOS(sentences)\n",
    "\n",
    "infile = open('CorpusPOS.txt', 'r')\n",
    "cPOS = infile.readlines()\n",
    "infile.close()\n",
    "(a, b, c, d, e) = calc_probabilities(cPOS)\n",
    "q1_output(a, b, c, d, e)\n",
    "\n",
    "Prob = {}\n",
    "infile = open('probabilities.txt', 'r')\n",
    "prob_text = infile.readlines()\n",
    "\n",
    "for sentence in prob_text:\n",
    "    keyValPair = sentence.split(\":\")\n",
    "    Prob[keyValPair[0]] = float(keyValPair[1][:-1])\n",
    "\n",
    "infile.close()\n",
    "\n",
    "posFeatures = minePOSPats(cPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Feature extract\n",
    "names = []\n",
    "F_features = []\n",
    "GRF_features = []\n",
    "WC_features = []\n",
    "labels = []\n",
    "\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './blogs-2000/female/'\n",
    "    else:\n",
    "        txtDir = './blogs-2000/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    blogs_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(blogs_gender))):\n",
    "        m = blogs_gender[i]\n",
    "        print(\"Processing: \", m)\n",
    "        print(\"Files:\", len(os.listdir(txtDir + m)))\n",
    "        for file in tqdm(os.listdir(txtDir + m), leave=False):\n",
    "            name = txtDir + m + '/' + file\n",
    "            text = gettext(name)\n",
    "            words = nltk.word_tokenize(text)\n",
    "            tags = nltk.pos_tag(words)\n",
    "            words_l = wordlemmatize(tags)\n",
    "\n",
    "            F_feature = F_measure(tags)\n",
    "            GRF_feature = Gender_Preferential_Features(words_l)\n",
    "            WC_feature = Word_Classes_Feature(words_l)\n",
    "\n",
    "            names.append(name)\n",
    "            F_features.append(F_feature)\n",
    "            GRF_features.append(GRF_feature)\n",
    "            WC_features.append(WC_feature)\n",
    "            labels.append(gender)\n",
    "\n",
    "WC_features_l = []\n",
    "GRF_features_l = []\n",
    "\n",
    "for i in range(len(WC_features[0])):\n",
    "    n = i\n",
    "    WC_features_l.append(getsingle(WC_features, n))\n",
    "\n",
    "for i in range(len(GRF_features[0])):\n",
    "    n = i\n",
    "    GRF_features_l.append(getsingle(GRF_features, n))\n",
    "\n",
    "map1 = {'name': names, 'label': labels, 'F_feature': F_features}\n",
    "\n",
    "for i in range(len(WC_features[0])):\n",
    "    key = 'WC_' + str(i + 1)\n",
    "    value = WC_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "for i in range(len(GRF_features[0])):\n",
    "    key = 'GRF_' + str(i + 1)\n",
    "    value = GRF_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "allofall = pd.DataFrame(map1)\n",
    "\n",
    "F_features_u = np.array(F_features)\n",
    "F_features_u = (F_features_u - np.mean(F_features_u)) / np.std(F_features_u)\n",
    "allofall['F_feature'] = F_features_u\n",
    "\n",
    "allofall.to_csv('allofall_blogs-2000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Get single\n",
    "WC_features_l = []\n",
    "for i in range(len(WC_features[0])):\n",
    "    n = i\n",
    "    WC_features_l.append(getsingle(WC_features, n))\n",
    "\n",
    "GRF_features_l = []\n",
    "for i in range(len(GRF_features[0])):\n",
    "    n = i\n",
    "    GRF_features_l.append(getsingle(GRF_features, n))\n",
    "\n",
    "POS_features_l = []\n",
    "for i in range(len(POS_features[0])):\n",
    "    n = i\n",
    "    POS_features_l.append(getsingle(POS_features, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Get DataFrame\n",
    "map1 = {'name': names, 'label': labels, 'F_feature': F_features}\n",
    "\n",
    "for i in range(len(WC_features[0])):\n",
    "    key = 'WC_' + str(i + 1)\n",
    "    value = WC_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "for i in range(len(GRF_features[0])):\n",
    "    key = 'GRF_' + str(i + 1)\n",
    "    value = GRF_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "for i in range(len(POS_features[0])):\n",
    "    key = 'POS_' + str(i + 1)\n",
    "    value = POS_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "allofall = pd.DataFrame(map1)\n",
    "F_features_u = np.array(F_features)\n",
    "F_features_u = (F_features_u - np.mean(F_features_u)) / np.std(F_features_u)\n",
    "allofall['F_feature'] = F_features_u\n",
    "\n",
    "allofall.to_csv('allofall_blogs.csv', index=False)\n",
    "genderbias = pd.read_csv('blogs_genderbias.csv')\n",
    "allofall['bias'], allofall['word ratio'] = genderbias['bias'], genderbias[\n",
    "    'word ratio']\n",
    "allofall.to_csv('blogs_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T08:20:58.286493Z",
     "start_time": "2019-08-09T08:17:42.076869Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#joint\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './postprocess_blogs/blogs/female/'\n",
    "        outdir = './blogs-2000/female/'\n",
    "    else:\n",
    "        txtDir = './postprocess_blogs/blogs/male/'\n",
    "        outdir = './blogs-2000/male/'\n",
    "\n",
    "    if not op.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    blogs_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(blogs_gender))):\n",
    "        m = blogs_gender[i]\n",
    "        handle = ''\n",
    "        print(\"Processing: \", m)\n",
    "        print(\"Files:\", len(os.listdir(txtDir + m)))\n",
    "        for file in tqdm(os.listdir(txtDir + m),leave=False):\n",
    "            name = txtDir + m + '/' + file\n",
    "            text = gettext(name)\n",
    "            handle = handle + ' ' + text\n",
    "        os.mkdir(op.join(outdir, m))\n",
    "        path = op.join(outdir, m, f'{m}.txt')\n",
    "        with open(path, 'w') as w:\n",
    "            w.write(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T11:26:49.585267Z",
     "start_time": "2019-08-09T11:20:04.248804Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#cut\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './blogs-2000/female/'\n",
    "    else:\n",
    "        txtDir = './blogs-2000/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    blogs_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(blogs_gender))):\n",
    "        m = blogs_gender[i]\n",
    "        print(\"Processing: \", m)\n",
    "        filepath = op.join(txtDir, m, f'{m}.txt')\n",
    "        text = gettext(filepath)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        imax = len(words) // 2000\n",
    "        for i in tqdm(range(imax),leave=False):\n",
    "            handle = words[i * 2000:(i + 1) * 2000]\n",
    "            handle = ' '.join(handle)\n",
    "            savepath = op.join(txtDir, m, f'{m}_{i+1}.txt')\n",
    "            with open(savepath, 'w') as w:\n",
    "                w.write(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:05:32.329785Z",
     "start_time": "2019-08-16T13:05:15.389239Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Mine POS patterns\n",
    "tagList = [\n",
    "    'NN', 'CC', 'LS', 'PDT', 'POS', 'SYM', 'NNS', 'NNP', 'NNPS', 'FW', 'CD',\n",
    "    'JJ', 'JJR', 'JJS', 'IN', 'TO', 'DT', 'EX', 'PRP', 'PRP$', 'WDT', 'WP',\n",
    "    'WP$', 'MD', 'VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG', 'RB', 'RBR', 'RBS',\n",
    "    'RP', 'WRB', 'UH', '.'\n",
    "]\n",
    "namelist = []\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './5000/female/'\n",
    "    else:\n",
    "        txtDir = './5000/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    handle_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(handle_gender))):\n",
    "        m = handle_gender[i]\n",
    "        path = op.join(txtDir, m)\n",
    "        text = gettext(path)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        name = op.join(txtDir, 'pos' + m)\n",
    "        CorpusPOS(sentences, name)\n",
    "        namelist.append(name)\n",
    "\n",
    "infile = open('CorpusPOS.txt', 'r')\n",
    "cPOS = infile.readlines()\n",
    "infile.close()\n",
    "(a, b, c, d, e, f, g) = calc_probabilities(cPOS)\n",
    "q1_output(a, b, c, d, e, f, g)\n",
    "\n",
    "Prob = {}\n",
    "infile = open('probabilities.txt', 'r')\n",
    "prob_text = infile.readlines()\n",
    "\n",
    "for sentence in prob_text:\n",
    "    keyValPair = sentence.split(\":\")\n",
    "    Prob[keyValPair[0]] = float(keyValPair[1][:-1])\n",
    "\n",
    "infile.close()\n",
    "\n",
    "posFeatures = minePOSPats(namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Word count and feature extract\n",
    "names = []\n",
    "F_features = []\n",
    "GRF_features = []\n",
    "WC_features = []\n",
    "labels = []\n",
    "lengths = []\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './5000/female/'\n",
    "    else:\n",
    "        txtDir = './5000/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    handle_gender = os.listdir(txtDir)\n",
    "    print(\"Files:\", len(handle_gender))\n",
    "    for m in handle_gender:\n",
    "        name = txtDir + m\n",
    "        text = gettext(name)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        tags = nltk.pos_tag(words)\n",
    "        words_l = wordlemmatize(tags)\n",
    "\n",
    "        length = len(words)\n",
    "        F_feature = F_measure(tags)\n",
    "        GRF_feature = Gender_Preferential_Features(words_l)\n",
    "        WC_feature = Word_Classes_Feature(words_l)\n",
    "\n",
    "        names.append(name)\n",
    "        lengths.append(length)\n",
    "        F_features.append(F_feature)\n",
    "        GRF_features.append(GRF_feature)\n",
    "        WC_features.append(WC_feature)\n",
    "        labels.append(gender)\n",
    "\n",
    "WC_features_l = []\n",
    "GRF_features_l = []\n",
    "\n",
    "for i in range(len(WC_features[0])):\n",
    "    n = i\n",
    "    WC_features_l.append(getsingle(WC_features, n))\n",
    "\n",
    "for i in range(len(GRF_features[0])):\n",
    "    n = i\n",
    "    GRF_features_l.append(getsingle(GRF_features, n))\n",
    "\n",
    "map1 = {\n",
    "    'name': names,\n",
    "    'label': labels,\n",
    "    'F_feature': F_features,\n",
    "    'word count': lengths\n",
    "}\n",
    "\n",
    "for i in range(len(WC_features[0])):\n",
    "    key = 'WC_' + str(i + 1)\n",
    "    value = WC_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "for i in range(len(GRF_features[0])):\n",
    "    key = 'GRF_' + str(i + 1)\n",
    "    value = GRF_features_l[i]\n",
    "    map1[key] = value\n",
    "\n",
    "allofall = pd.DataFrame(map1)\n",
    "\n",
    "F_features_u = np.array(F_features)\n",
    "F_features_u = (F_features_u - np.mean(F_features_u)) / np.std(F_features_u)\n",
    "allofall['F_feature'] = F_features_u\n",
    "\n",
    "allofall.to_csv('allofall_5000.csv', index=False)\n",
    "genderbias = pd.read_csv('5000_genderbias.csv')\n",
    "allofall['bias'], allofall['word ratio'] = genderbias['bias'], genderbias[\n",
    "    'word ratio']\n",
    "allofall.to_csv('5000_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T16:34:02.246741Z",
     "start_time": "2019-08-16T13:51:10.753785Z"
    }
   },
   "outputs": [],
   "source": [
    "#new feature\n",
    "swnPos, swnNeg = preforsen()\n",
    "baseFs = []\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './5000/female/'\n",
    "    else:\n",
    "        txtDir = './5000/male/'\n",
    "\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    handle_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(handle_gender))):\n",
    "        m = handle_gender[i]\n",
    "        text = op.join(txtDir, m)\n",
    "        text = gettext(text)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        basefeature = baseFeatures(text, words, sentences, swnPos, swnNeg)\n",
    "        baseFs.append(basefeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allofall['word count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T07:48:41.347377Z",
     "start_time": "2019-08-03T05:14:42.733070Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shorten text\n",
    "#length_list = [200, 400, 600, 800, 1200, 1600, 2000, 27850]\n",
    "length_list = [12000, 14000, 16000, 18000]\n",
    "\n",
    "for l in length_list:\n",
    "    print('\\nPresent length:{}'.format(l))\n",
    "    for gender in [0, 1]:\n",
    "        if gender == 0:\n",
    "            txtDir = './5000/female/'\n",
    "            saveDir = f'./5000-{l}/female/'\n",
    "        else:\n",
    "            txtDir = './5000/male/'\n",
    "            saveDir = f'./5000-{l}/male/'\n",
    "        if not op.isdir(f'./5000-{l}'):\n",
    "            os.mkdir(f'./5000-{l}')\n",
    "        if not op.isdir(saveDir):\n",
    "            os.mkdir(saveDir)\n",
    "        print(\"Processing gender: {}\".format(txtDir))\n",
    "        handle_gender = os.listdir(txtDir)\n",
    "        print(\"Files:\", len(handle_gender))\n",
    "        for m in tqdm(handle_gender):\n",
    "            name = txtDir + m\n",
    "            text = gettext(name)\n",
    "            words = nltk.word_tokenize(text)\n",
    "            if len(words) < (l + 1):\n",
    "                continue\n",
    "            words = words[:l]\n",
    "            text = ''\n",
    "            for word in words:\n",
    "                text = text + word + ' '\n",
    "            with open(op.join(saveDir, m), 'w') as f:\n",
    "                f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T16:37:45.182486Z",
     "start_time": "2019-08-10T15:59:11.513096Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#cut\n",
    "for gender in [0, 1]:\n",
    "    if gender == 0:\n",
    "        txtDir = './5000/female/'\n",
    "        outdir = './5000-2000/female/'\n",
    "    else:\n",
    "        txtDir = './5000/male/'\n",
    "        outdir = './5000-2000/male/'\n",
    "\n",
    "    if not op.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    print(\"Processing gender: {}\".format(txtDir))\n",
    "    blogs_gender = os.listdir(txtDir)\n",
    "    for i in tqdm(range(0, len(blogs_gender))):\n",
    "        m = blogs_gender[i]\n",
    "        filepath = op.join(txtDir, m)\n",
    "        text = gettext(filepath)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        imax = len(words) // 2000\n",
    "        for i in tqdm(range(imax),leave=False):\n",
    "            handle = words[i * 2000:(i + 1) * 2000]\n",
    "            handle = ' '.join(handle)\n",
    "            savepath = op.join(outdir, f'{m[:-4]}_{i+1}.txt')\n",
    "            with open(savepath, 'w') as w:\n",
    "                w.write(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T05:26:23.831451Z",
     "start_time": "2019-08-11T00:25:43.939754Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature extract\n",
    "length_list = [2000]\n",
    "for l in length_list:\n",
    "    names = []\n",
    "    F_features = []\n",
    "    GRF_features = []\n",
    "    WC_features = []\n",
    "    labels = []\n",
    "    print('\\nPresent length:{}'.format(l))\n",
    "    for gender in [0, 1]:\n",
    "        if gender == 0:\n",
    "            txtDir = f'./5000-{l}/female/'\n",
    "        else:\n",
    "            txtDir = f'./5000-{l}/male/'\n",
    "\n",
    "        print(\"Processing gender: {}\".format(txtDir))\n",
    "        handle_gender = os.listdir(txtDir)\n",
    "        print(\"Files:\", len(handle_gender))\n",
    "        for m in tqdm(handle_gender):\n",
    "            name = txtDir + m\n",
    "            text = gettext(name)\n",
    "            words = nltk.word_tokenize(text)\n",
    "            tags = nltk.pos_tag(words)\n",
    "            words_l = wordlemmatize(tags)\n",
    "\n",
    "            F_feature = F_measure(tags)\n",
    "            GRF_feature = Gender_Preferential_Features(words_l)\n",
    "            WC_feature = Word_Classes_Feature(words_l)\n",
    "\n",
    "            names.append(name)\n",
    "            F_features.append(F_feature)\n",
    "            GRF_features.append(GRF_feature)\n",
    "            WC_features.append(WC_feature)\n",
    "            labels.append(gender)\n",
    "\n",
    "    WC_features_l = []\n",
    "    GRF_features_l = []\n",
    "\n",
    "    for i in range(len(WC_features[0])):\n",
    "        n = i\n",
    "        WC_features_l.append(getsingle(WC_features, n))\n",
    "\n",
    "    for i in range(len(GRF_features[0])):\n",
    "        n = i\n",
    "        GRF_features_l.append(getsingle(GRF_features, n))\n",
    "\n",
    "    map1 = {'name': names, 'label': labels, 'F_feature': F_features}\n",
    "\n",
    "    for i in range(len(WC_features[0])):\n",
    "        key = 'WC_' + str(i + 1)\n",
    "        value = WC_features_l[i]\n",
    "        map1[key] = value\n",
    "\n",
    "    for i in range(len(GRF_features[0])):\n",
    "        key = 'GRF_' + str(i + 1)\n",
    "        value = GRF_features_l[i]\n",
    "        map1[key] = value\n",
    "\n",
    "    allofall = pd.DataFrame(map1)\n",
    "\n",
    "    F_features_u = np.array(F_features)\n",
    "    F_features_u = (F_features_u -\n",
    "                    np.mean(F_features_u)) / np.std(F_features_u)\n",
    "    allofall['F_feature'] = F_features_u\n",
    "\n",
    "    allofall.to_csv(f'allofall_5000-{l}.csv', index=False)\n",
    "    #genderbias = pd.read_csv(f'5000-{l}_genderbias.csv')\n",
    "    #allofall['bias'], allofall['word ratio'] = genderbias['bias'], genderbias['word ratio']\n",
    "    #allofall.to_csv(f'5000-{l}_features.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T07:32:09.032856Z",
     "start_time": "2019-08-17T07:32:09.027441Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(clf, data, target):\n",
    "    scores_clf_cv = cross_val_score(clf,\n",
    "                                    data.values,\n",
    "                                    target.values.reshape(-1, ),\n",
    "                                    cv=5)\n",
    "    print(scores_clf_cv)\n",
    "    print(\"Accuracy: %0.6f (+/- %0.3f)\" %\n",
    "          (scores_clf_cv.mean(), scores_clf_cv.std() * 2))\n",
    "    return scores_clf_cv.mean(), clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T08:20:30.145069Z",
     "start_time": "2019-08-04T08:18:54.859465Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train\n",
    "allf = []\n",
    "allf_i = []\n",
    "gb = []\n",
    "gb_i = []\n",
    "without = []\n",
    "without_i = []\n",
    "featurelist = []\n",
    "length_list = [12000, 14000, 16000, 18000]\n",
    "list1 = ['occu', 'emo', 'verb']\n",
    "\n",
    "for l in length_list:\n",
    "    allofall = pd.read_csv(f'allofall_5000-{l}.csv')\n",
    "    genderbias = pd.read_csv(f'5000-{l}_genderbias.csv')\n",
    "    allofall['bias'], allofall['word ratio'] = genderbias['bias'], genderbias[\n",
    "        'word ratio']\n",
    "\n",
    "    for j in range(3):\n",
    "        genderbias = pd.read_csv(f'5000-{l}_genderbias-{list1[j]}.csv')\n",
    "        allofall[f'bias-{list1[j]}'], allofall[\n",
    "            f'word ratio{list1[j]}'] = genderbias['bias'], genderbias[\n",
    "                'word ratio']\n",
    "\n",
    "    allofall.to_csv(f'5000-{l}_features.csv', index=False)\n",
    "    data, target = allofall.drop(\n",
    "        columns=['name', 'label']).iloc[:].values, allofall['label'].values\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data,\n",
    "                                                        target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True)\n",
    "    data, target = pd.concat(\n",
    "        (pd.DataFrame(train_X), pd.DataFrame(test_X))), pd.concat(\n",
    "            (pd.DataFrame(train_y), pd.DataFrame(test_y)))\n",
    "    data1 = data.iloc[:, :-6]\n",
    "    data2 = data.iloc[:, :-8]\n",
    "    data3 = data.iloc[:, -8:-6]\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=800,\n",
    "                                     max_leaf_nodes=50,\n",
    "                                     n_jobs=-1)\n",
    "\n",
    "    print('\\n Length:{}'.format(l))\n",
    "    a, b = train(rnd_clf, data1, target)\n",
    "    allf.append(a)\n",
    "    allf_i.append([b])\n",
    "    a, b = train(rnd_clf, data2, target)\n",
    "    without.append(a)\n",
    "    without_i.append([b])\n",
    "    a, b = train(rnd_clf, data3, target)\n",
    "    gb.append(a)\n",
    "    gb_i.append([b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-04T08:23:28.766155Z",
     "start_time": "2019-08-04T08:23:28.697208Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#length vs accuracy\n",
    "df = pd.read_csv('length_v_acc_and_im.csv')\n",
    "df = df.set_index('length')\n",
    "allofall = pd.read_csv('5000-10000_features.csv')\n",
    "\n",
    "allf_ii = [0] * len(length_list)\n",
    "without_ii = [0] * len(length_list)\n",
    "gb_ii = [0] * len(length_list)\n",
    "name = allofall.columns[2:]\n",
    "for i in range(len(length_list)):\n",
    "    allf_ii[i] = sorted(list(zip(name, allf_i[i][0])),\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True)\n",
    "    without_ii[i] = sorted(list(zip(name[:-2], without_i[i][0])),\n",
    "                           key=lambda x: x[1],\n",
    "                           reverse=True)\n",
    "    gb_ii[i] = sorted(list(zip(name[-2:], gb_i[i][0])),\n",
    "                      key=lambda x: x[1],\n",
    "                      reverse=True)\n",
    "\n",
    "allf_iii = [0] * len(length_list)\n",
    "without_iii = [0] * len(length_list)\n",
    "name = ['F', 'WC', 'GRF', 'bias', 'word ratio']\n",
    "for i in range(len(length_list)):\n",
    "    temp = allf_i[i]\n",
    "    handle = [0] * 5\n",
    "    handle[0] = temp[0][0]\n",
    "    handle[1] = sum(temp[0][1:24])\n",
    "    handle[2] = sum(temp[0][24:34])\n",
    "    handle[3:] = temp[0][-2:]\n",
    "    handle = sorted(zip(name, handle), key=lambda x: x[1], reverse=True)\n",
    "    allf_iii[i] = handle\n",
    "    temp = without_i[i]\n",
    "    handle = [0] * 3\n",
    "    handle[0] = temp[0][0]\n",
    "    handle[1] = sum(temp[0][1:24])\n",
    "    handle[2] = sum(temp[0][24:34])\n",
    "    handle = sorted(zip(name[:-2], handle), key=lambda x: x[1], reverse=True)\n",
    "    without_iii[i] = handle\n",
    "\n",
    "start = 8\n",
    "df1 = df.iloc[:8, :]\n",
    "df2 = df.iloc[:len(length_list),:]\n",
    "df2.index = length_list\n",
    "df3 = df.iloc[8:, :]\n",
    "df = pd.concat([df1, df2, df3])\n",
    "for i in range(len(length_list)):\n",
    "    df.iloc[start + i, 0] = allf[i]\n",
    "    df.iloc[start + i, 3] = without[i]\n",
    "    df.iloc[start + i, 6] = gb[i]\n",
    "    df.iloc[start + i, 1] = [allf_ii[i][:5]]\n",
    "    df.iloc[start + i, 4] = [without_ii[i][:5]]\n",
    "    df.iloc[start + i, 7] = [gb_ii[i]]\n",
    "    df.iloc[start + i, 2] = [allf_iii[i]]\n",
    "    df.iloc[start + i, 5] = [without_iii[i]]\n",
    "#df.to_csv('top5_sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10wincv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(1, 11):\n",
    "    fname = str(10 + i) + '_win_5000_genderbias.csv'\n",
    "    genderbias = pd.read_csv(fname)\n",
    "    allofall['bias'], allofall['word ratio'] = genderbias['bias'], genderbias[\n",
    "        'word ratio']\n",
    "    data, target = allofall.drop(columns=['name', 'label', 'word count']\n",
    "                                 ).iloc[:].values, allofall['label'].values\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data,\n",
    "                                                        target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True)\n",
    "    data, target = pd.concat(\n",
    "        (pd.DataFrame(train_X), pd.DataFrame(test_X))), pd.concat(\n",
    "            (pd.DataFrame(train_y), pd.DataFrame(test_y)))\n",
    "\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=1000,\n",
    "                                     max_leaf_nodes=50,\n",
    "                                     n_jobs=-1)\n",
    "    scores_rnd_clf_cv = cross_val_score(rnd_clf,\n",
    "                                        data.values,\n",
    "                                        target.values.reshape(-1, ),\n",
    "                                        cv=5)\n",
    "    print('Windows:%.2f'%(i))\n",
    "    print(scores_rnd_clf_cv)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" %\n",
    "          (scores_rnd_clf_cv.mean(), scores_rnd_clf_cv.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:06:09.728700Z",
     "start_time": "2019-07-31T09:05:16.451354Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "allf = []\n",
    "allf_i = []\n",
    "bias1 = []\n",
    "bias1_i = []\n",
    "bias2 = []\n",
    "bias2_i = []\n",
    "bias3 = []\n",
    "bias3_i = []\n",
    "list1 = ['occu', 'emo', 'verb']\n",
    "\n",
    "for l in length_list:\n",
    "    allofall = pd.read_csv(f'allofall_5000-{l}.csv')\n",
    "    for j in range(3):\n",
    "        genderbias = pd.read_csv(f'5000-{l}_genderbias-{list1[j]}.csv')\n",
    "        allofall[f'bias-{list1[j]}'], allofall[\n",
    "            f'word ratio{list1[j]}'] = genderbias['bias'], genderbias[\n",
    "                'word ratio']\n",
    "    allofall.to_csv(f'5000-{l}_features.csv', index=False)\n",
    "    data, target = allofall.drop(\n",
    "        columns=['name', 'label']).iloc[:].values, allofall['label'].values\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data,\n",
    "                                                        target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True)\n",
    "    data, target = pd.concat(\n",
    "        (pd.DataFrame(train_X), pd.DataFrame(test_X))), pd.concat(\n",
    "            (pd.DataFrame(train_y), pd.DataFrame(test_y)))\n",
    "    data1 = data.iloc[:, :-6]\n",
    "    data2 = data.iloc[:, :-8]\n",
    "    data3 = data.iloc[:, -8:-6]\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=800,\n",
    "                                     max_leaf_nodes=50,\n",
    "                                     n_jobs=-1)\n",
    "\n",
    "    print('\\n Length:{}'.format(l))\n",
    "    a, b = train(rnd_clf, data1, target)\n",
    "    allf.append(a)\n",
    "    allf_i.append([b])\n",
    "    a, b = train(rnd_clf, data2, target)\n",
    "    bias1.append(scores_rnd_clf_cv.mean())\n",
    "    bias1_i.append([rnd_clf.feature_importances_])\n",
    "    a, b = train(rnd_clf, data3, target)\n",
    "    bias2.append(scores_rnd_clf_cv.mean())\n",
    "    bias2_i.append([rnd_clf.feature_importances_])\n",
    "\n",
    "    bias3.append(scores_rnd_clf_cv.mean())\n",
    "    bias3_i.append([rnd_clf.feature_importances_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T06:32:58.307048Z",
     "start_time": "2019-08-11T06:28:57.755875Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_leaf_nodes=32),\n",
    "                             n_estimators=200,\n",
    "                             algorithm=\"SAMME.R\",\n",
    "                             learning_rate=1,\n",
    "                             random_state=42)\n",
    "ada_clf.fit(train_X, train_y)\n",
    "ada_clf.score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T06:40:06.152336Z",
     "start_time": "2019-08-11T06:39:24.915656Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=1000,\n",
    "                                 max_leaf_nodes=64,\n",
    "                                 n_jobs=-1)\n",
    "gbrt = GradientBoostingRegressor(max_leaf_nodes=64,\n",
    "                                 n_estimators=10,\n",
    "                                 learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:00:14.783767Z",
     "start_time": "2019-08-10T15:00:10.934153Z"
    }
   },
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(train_X, train_y)\n",
    "\n",
    "[estimator.score(test_X, test_y) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:01:48.935214Z",
     "start_time": "2019-08-10T15:01:45.197013Z"
    }
   },
   "outputs": [],
   "source": [
    "named_estimators = [\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:02:00.313836Z",
     "start_time": "2019-08-10T15:02:00.273055Z"
    }
   },
   "outputs": [],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:02:33.609300Z",
     "start_time": "2019-08-10T15:02:33.595850Z"
    }
   },
   "outputs": [],
   "source": [
    "voting_clf.set_params(extra_trees_clf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:02:46.275221Z",
     "start_time": "2019-08-10T15:02:46.272559Z"
    }
   },
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[1]\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:02:58.061027Z",
     "start_time": "2019-08-10T15:02:58.058053Z"
    }
   },
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:03:21.517948Z",
     "start_time": "2019-08-10T15:03:21.477089Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:03:29.281308Z",
     "start_time": "2019-08-10T15:03:29.133528Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)\n",
    "rnd_forest_blender.score(X_val_predictions,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T15:03:34.684814Z",
     "start_time": "2019-08-10T15:03:34.679655Z"
    }
   },
   "outputs": [],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T07:30:25.303005Z",
     "start_time": "2019-08-17T07:30:25.252927Z"
    }
   },
   "outputs": [],
   "source": [
    "f = ['bias', 'word ratio', 'base_15', 'GRF_8', 'WC_2', 'GRF_1']\n",
    "allofall = pd.read_csv('newnewnew.csv')\n",
    "data, target = allofall.drop(\n",
    "    columns=['name', 'label', 'word count']).iloc[:], allofall['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T07:31:12.394097Z",
     "start_time": "2019-08-17T07:31:12.389259Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 = data[f]\n",
    "clf = RandomForestClassifier(n_estimators=800, max_leaf_nodes=64, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T07:32:20.890279Z",
     "start_time": "2019-08-17T07:32:14.284674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.728 0.762 0.78  0.771 0.76 ]\n",
      "Accuracy: 0.760200 (+/- 0.035)\n"
     ]
    }
   ],
   "source": [
    "a,b = train(clf,data1,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T07:32:45.280307Z",
     "start_time": "2019-08-17T07:32:36.280883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.751 0.773 0.8   0.803 0.78 ]\n",
      "Accuracy: 0.781400 (+/- 0.038)\n"
     ]
    }
   ],
   "source": [
    "a,b = train(clf,data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "316.85px",
    "left": "1418px",
    "right": "50px",
    "top": "120px",
    "width": "317px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
